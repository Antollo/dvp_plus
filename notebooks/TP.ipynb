{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lcmr_ext.encoder.dino_v2_encoder import DinoV2Encoder\n",
    "from lcmr_ext.loss import ImageMaeLoss, SceneLoss\n",
    "from lcmr_ext.modeler import ConditionalDETRModeler, EfdModuleConfig, EfdModuleMode, ModelerConfig\n",
    "from lcmr_ext.renderer.renderer2d import PyTorch3DRenderer2D\n",
    "from lcmr_ext.utils.sample_efd import ellipse_efd, heart_efd, square_efd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from transformers.models.conditional_detr import ConditionalDetrConfig\n",
    "\n",
    "from lcmr.dataset import  DatasetOptions, EfdGeneratorOptions, RandomDataset\n",
    "from lcmr.grammar.scene_data import SceneData\n",
    "from lcmr.reconstruction_model import ReconstructionModel\n",
    "from lcmr.utils.colors import colors\n",
    "from lcmr.utils.presentation import display_img, make_img_grid\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "raster_size = (128, 128)\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "show_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650005a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "    \n",
    "choices = torch.cat([heart_efd()[None], square_efd()[None], ellipse_efd()[None]], dim=0)\n",
    "\n",
    "train_options = DatasetOptions(\n",
    "    raster_size=raster_size,\n",
    "    n_samples=50_000,\n",
    "    n_objects=4,\n",
    "    Renderer=PyTorch3DRenderer2D,\n",
    "    renderer_device=device,\n",
    "    n_jobs=1,\n",
    "    return_images=True,\n",
    "    efd_options=EfdGeneratorOptions(choices=choices),\n",
    ")\n",
    "\n",
    "val_options = DatasetOptions(\n",
    "    raster_size=raster_size,\n",
    "    n_samples=5_000,\n",
    "    n_objects=4,\n",
    "    Renderer=PyTorch3DRenderer2D,\n",
    "    renderer_device=device,\n",
    "    n_jobs=1,\n",
    "    return_images=True,\n",
    "    efd_options=EfdGeneratorOptions(choices=choices),\n",
    ")\n",
    "\n",
    "test_options = DatasetOptions(\n",
    "    raster_size=raster_size,\n",
    "    n_samples=5_000,\n",
    "    n_objects=4,\n",
    "    Renderer=PyTorch3DRenderer2D,\n",
    "    renderer_device=device,\n",
    "    n_jobs=1,\n",
    "    return_images=True,\n",
    "    efd_options=EfdGeneratorOptions(choices=choices),\n",
    ")\n",
    "\n",
    "train_dataset = RandomDataset(train_options)\n",
    "val_dataset = RandomDataset(val_options)\n",
    "test_dataset = RandomDataset(test_options)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b43043",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=train_dataset.collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ec698",
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = PyTorch3DRenderer2D(raster_size, background_color=colors.black, return_alpha=True, n_verts=64, device=device)\n",
    "\n",
    "encoder = DinoV2Encoder(input_size=(126, 126)).to(device)\n",
    "modeler = ConditionalDETRModeler(\n",
    "    config=ModelerConfig(\n",
    "        encoder_feature_dim=768,\n",
    "        use_single_scale=True,\n",
    "        use_confidence=True,\n",
    "        efd_module_config=EfdModuleConfig(order=16, num_prototypes=3, mode=EfdModuleMode.PrototypeAttention),\n",
    "    ),\n",
    "    detr_config=ConditionalDetrConfig(num_queries=8, dropout=0),\n",
    ").to(device)\n",
    "\n",
    "model = ReconstructionModel(encoder, modeler, renderer)\n",
    "\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(list(modeler.parameters()), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.modeler.to_efd.prototypes[:] = torch.load(\"prototypes.pt\").to(device)\n",
    "modeler.to_efd.prototypes.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f04bd9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiled_model = torch.compile(model)\n",
    "compiled_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1973f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"TP\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "loss_fn_image = ImageMaeLoss().to(device)\n",
    "loss_fn_scene = SceneLoss().to(device)\n",
    "\n",
    "val_loss_fn_image = ImageMaeLoss().to(device)\n",
    "val_loss_fn_scene = SceneLoss().to(device)\n",
    "\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    loss_fn_scene.reset()\n",
    "    loss_fn_image.reset()\n",
    "\n",
    "    modeler.train()\n",
    "    for j, target in enumerate(epoch_bar := tqdm(train_dataloader, desc=\"Epoch\")):\n",
    "        target: SceneData = target.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        pred: SceneData = compiled_model(target.image_rgb, render=False)\n",
    "        loss = 0\n",
    "        \n",
    "        target = pred.clone().detach()\n",
    "        target.scene.layer.object.appearance.color = target.scene.layer.object.appearance.color.detach()\n",
    "        target.scene.layer.object.appearance.confidence = target.scene.layer.object.appearance.confidence.detach()\n",
    "        target.scene.layer.object.efd = target.scene.layer.object.efd.detach()\n",
    "        target.scene.layer.object.transformation.scale = target.scene.layer.object.transformation.scale.detach()\n",
    "        target.scene.layer.object.transformation.translation = target.scene.layer.object.transformation.translation.detach()\n",
    "        n_objects = 8\n",
    "        target.scene.layer.object.transformation.rotation_vec = torch.nn.functional.normalize(torch.rand(batch_size * n_objects, 2) * 2 - 1, dim=-1).view(batch_size, 1, n_objects, 2)\n",
    "        target.scene.layer.object.appearance.confidence = target.scene.layer.object.appearance.confidence.round()\n",
    "        target.scene.layer.object.efd = model.modeler.to_efd.prototypes[torch.randint(0, 3, [128, 1, 8])].detach()\n",
    "        with torch.no_grad():\n",
    "            target: SceneData = renderer.render(target.scene)\n",
    "            \n",
    "        pred: SceneData = compiled_model(target.image_rgb, render=False)\n",
    "        \n",
    "        loss = loss + loss_fn_scene(target, pred)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            epoch_bar.set_postfix({\"scene_image\": str(loss_fn_image), \"scene_loss\": str(loss_fn_scene)})\n",
    "\n",
    "    modeler.eval()\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        pred: SceneData = renderer.render(compiled_model(target.image_rgb, render=False).scene)\n",
    "\n",
    "        img_grid = make_img_grid((pred.image_rgb_top, target.image_rgb_top))\n",
    "        writer.add_image(\"visualization\", img_grid.permute(2, 0, 1), global_step=epoch)\n",
    "\n",
    "        if epoch % show_step == 0:\n",
    "            display_img(img_grid)\n",
    "\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        loss_fn_image.show()\n",
    "        loss_fn_scene.show()\n",
    "\n",
    "        val_loss_fn_image.reset()\n",
    "        val_loss_fn_scene.reset()\n",
    "\n",
    "        for j, target in enumerate(epoch_bar := tqdm(val_dataloader, desc=\"Epoch\")):\n",
    "            target: SceneData = target.to(device)\n",
    "\n",
    "            pred: SceneData = renderer.render(compiled_model(target.image_rgb, render=False).scene)\n",
    "            val_loss_fn_image(target, pred)\n",
    "            val_loss_fn_scene(target, pred)\n",
    "\n",
    "            epoch_bar.set_postfix({\"scene_image\": str(val_loss_fn_image), \"scene_loss\": str(val_loss_fn_scene)})\n",
    "\n",
    "        val_loss_fn_image.show()\n",
    "        val_loss_fn_scene.show()\n",
    "\n",
    "    writer.add_scalars(\n",
    "        \"image_loss\",\n",
    "        {\"training\": loss_fn_image.compute(), \"validation\": val_loss_fn_image.compute()},\n",
    "        global_step=epoch,\n",
    "    )\n",
    "    writer.add_scalars(\n",
    "        \"scene_loss\",\n",
    "        {\"training\": loss_fn_image.compute(), \"validation\": val_loss_fn_image.compute()},\n",
    "        global_step=epoch,\n",
    "    )\n",
    "\n",
    "    torch.save(modeler, f\"{log_dir}/model_{epoch}.pt\")\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
