{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lcmr_ext.renderer.renderer2d import PyTorch3DRenderer2D\n",
    "from lcmr_ext.utils.sample_efd import ellipse_efd, heart_efd, square_efd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lcmr.dataset import  DatasetOptions, EfdGeneratorOptions, RandomDataset\n",
    "from lcmr.grammar.scene_data import SceneData\n",
    "from lcmr.reconstruction_model import ReconstructionModel\n",
    "from lcmr.utils.colors import colors\n",
    "from lcmr.utils.presentation import display_img, make_img_grid\n",
    "\n",
    "from lcmr.grammar import Scene\n",
    "from torch.nn.functional import l1_loss\n",
    "\n",
    "from torchmetrics.aggregation import MeanMetric\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "raster_size = (128, 128)\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "show_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650005a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "    \n",
    "choices = torch.cat([heart_efd()[None], square_efd()[None], ellipse_efd()[None]], dim=0)\n",
    "\n",
    "train_options = DatasetOptions(\n",
    "    raster_size=raster_size,\n",
    "    n_samples=50_000,\n",
    "    n_objects=4,\n",
    "    Renderer=PyTorch3DRenderer2D,\n",
    "    renderer_device=device,\n",
    "    n_jobs=1,\n",
    "    return_images=True,\n",
    "    efd_options=EfdGeneratorOptions(choices=choices),\n",
    ")\n",
    "\n",
    "val_options = DatasetOptions(\n",
    "    raster_size=raster_size,\n",
    "    n_samples=5_000,\n",
    "    n_objects=4,\n",
    "    Renderer=PyTorch3DRenderer2D,\n",
    "    renderer_device=device,\n",
    "    n_jobs=1,\n",
    "    return_images=True,\n",
    "    efd_options=EfdGeneratorOptions(choices=choices),\n",
    ")\n",
    "\n",
    "test_options = DatasetOptions(\n",
    "    raster_size=raster_size,\n",
    "    n_samples=5_000,\n",
    "    n_objects=4,\n",
    "    Renderer=PyTorch3DRenderer2D,\n",
    "    renderer_device=device,\n",
    "    n_jobs=1,\n",
    "    return_images=True,\n",
    "    efd_options=EfdGeneratorOptions(choices=choices),\n",
    ")\n",
    "\n",
    "train_dataset = RandomDataset(train_options)\n",
    "val_dataset = RandomDataset(val_options)\n",
    "test_dataset = RandomDataset(test_options)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b43043",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=train_dataset.collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c21c9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./object_centric_library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cefb373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_centric_library.models.monet.model import Monet\n",
    "from object_centric_library.utils.viz import make_recon_img\n",
    "\n",
    "\n",
    "width, height = 128, 128\n",
    "monet_config = OmegaConf.load(\"./object_centric_library/config/model/monet.yaml\").model\n",
    "\n",
    "monet_config.num_slots = 5\n",
    "monet_config.decoder_params.w_broadcast = width + 8\n",
    "monet_config.decoder_params.h_broadcast = height + 8\n",
    "monet_config.num_blocks_unet = 6\n",
    "del monet_config[\"_target_\"]\n",
    "\n",
    "model = Monet(**monet_config, width=width, height=height).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83ae0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(list(model.parameters()), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1973f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"MONet\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "train_loss = MeanMetric()\n",
    "train_mae = MeanMetric()\n",
    "val_mae = MeanMetric()\n",
    "\n",
    "for epoch in (bar := range(0, epochs)):\n",
    "    train_loss.reset()\n",
    "    train_mae.reset()\n",
    "\n",
    "    model.train()\n",
    "    for j, target in enumerate(tqdm(train_dataloader)):\n",
    "        batch_size = len(target)\n",
    "        dataset_size = len(train_dataloader)\n",
    "        total_examples = batch_size * (epoch * dataset_size + j)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        target = target.to(device)\n",
    "\n",
    "\n",
    "        pred = model(target.image_rgb.permute(0, 3, 1, 2))\n",
    "        pred_img = make_recon_img(pred[\"slot\"], pred[\"mask\"]).permute(0, 2, 3, 1)\n",
    "        pred_alpha = pred[\"mask\"].permute(0, 1, 3, 4, 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mae = l1_loss(pred_img, target.image_rgb)\n",
    "            train_mae.update(mae.mean().cpu().item())\n",
    "\n",
    "        loss = pred[\"loss\"]\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    val_mae.reset()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_grid = make_img_grid((pred_img[:8], target.image_rgb_top))\n",
    "        writer.add_image(\"visualization\", img_grid.permute(2, 0, 1), global_step=epoch)\n",
    "\n",
    "        if epoch % show_step == 0:\n",
    "            display_img(img_grid)\n",
    "\n",
    "        val_mse = []\n",
    "        for target_scene in val_dataloader:\n",
    "            target_scene: Scene = target_scene.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            pred = model(target.image_rgb.permute(0, 3, 1, 2))\n",
    "            pred_img = make_recon_img(pred[\"slot\"], pred[\"mask\"]).permute(0, 2, 3, 1)\n",
    "\n",
    "            mae = l1_loss(pred_img, target.image_rgb)\n",
    "            val_mae.update(mae.mean().cpu().item())\n",
    "\n",
    "        print(f\"train loss: {train_mae.compute():.4f}\\tval loss: {val_mae.compute():.4f}\")\n",
    "    \n",
    "    writer.add_scalars(\n",
    "        \"image_loss\",\n",
    "        {\"training\": train_mae.compute(), \"validation\": val_mae.compute()},\n",
    "        global_step=epoch,\n",
    "    )\n",
    "    \n",
    "    torch.save(model, f\"{log_dir}/model_{epoch}.pt\")\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
